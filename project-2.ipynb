{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91a10dbd",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "## Hanan Sukenik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee17358",
   "metadata": {},
   "source": [
    "### Loading the Data into Kafka\n",
    "\n",
    "We will begin our work by using a Docker Compose .yml file to configure and start multiple Docker containers needed for our project. In this case, we used a Docker Compose file containing Zookeeper, Kafka, Cloudera (HDFS), Spark and the midsw250 base container.\n",
    "We will examine the Docker-Compose file, spin up the cluster and examine the kafka logs. Finally, we will make sure all of the containers are up.\n",
    "\n",
    "```bash\n",
    "cp ../course-content/08-Querying-Data/docker-compose.yml .\n",
    "vim docker-compose.yml \n",
    "docker-compose up -d\n",
    "docker-compose logs -f kafka\n",
    "docker-compose ps\n",
    "```\n",
    "\n",
    "Next, we will get our data file and examine it, first by using \"cat\" and then by using jq to examine the json file parsed into seperate lines:\n",
    "```bash\n",
    "curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp\n",
    "cat assessment-attempts-20180128-121051-nested.json\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-hanansuk/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c\"\n",
    "```\n",
    "\n",
    "Next, we will create a new topic on Kafka, that represents the messages we are about to send to the topic. In this case, the topic is \"assessments\", as the messages we represent different assessments. We will also make sure the topic was created successfuly, and then publish the parsed json file using the Kafka Console Producer.\n",
    "Finally, we will (again) make sure the containers are still running.\n",
    "```bash\n",
    "docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "docker-compose exec kafka kafka-topics --describe --topic assessments --zookeeper zookeeper:32181\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-hanansuk/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments && echo 'Produced assessments.'\"\n",
    "docker-compose ps\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e16085",
   "metadata": {},
   "source": [
    "### From Kafka to a Spark Dataframe\n",
    "\n",
    "Now, we will run Hadoop and Spark using the Hadoop & Spark containers:\n",
    "```bash\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "docker-compose exec spark pyspark\n",
    "```\n",
    "\n",
    "And read the messages from Kafka:\n",
    "```python\n",
    "assessments= spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "  .option(\"subscribe\",\"assessments\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"endingOffsets\", \"latest\") \\\n",
    "  .load() \n",
    "```\n",
    "Next, we will examine the Schema of the data and take an initial looks at it:\n",
    "```python\n",
    "assessments.printSchema()\n",
    "assessments.show()\n",
    "```\n",
    "It seems like the only column we need here is \"value\" one- so we will cast it as a string and extract it to a seperate variable, while examining the schema and the data again, and caching it to avoid warning messages later on:\n",
    "```python\n",
    "assessments_as_strings = assessments.selectExpr(\"CAST(value AS STRING)\")\n",
    "assessments_as_strings.show()\n",
    "assessments_as_strings.printSchema()\n",
    "assessments_as_strings.cache()\n",
    "```\n",
    "Now that the data is properly read into Spark, it's time to begin unwrapping it, using the json module, to examine the first message:\n",
    "```python\n",
    "import json\n",
    "first_assessment = json.loads(assessments_as_strings.select('value').take(1)[0].value)\n",
    "first_assessment\n",
    "first_assessment.keys\n",
    "```\n",
    "\n",
    "It seems like the assessments are rather messy, as they contain nested jsons for the different questions of each assessment. Therefore, we will use json.loads to pretty print the assessment and really look into it:\n",
    "```python\n",
    "print(json.dumps(first_assessment, indent=4, sort_keys=True))\n",
    "```\n",
    "\n",
    "Now we will want to map the messages onto an RDD(Spark's Distributed Dataset), apply a map to it and convert it back to a Spark dataframe:\n",
    "```python\n",
    "from pyspark.sql import Row\n",
    "assess_df = assessments_as_strings.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()\n",
    "assess_df.show()\n",
    "assess_df.printSchema()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d67d8d",
   "metadata": {},
   "source": [
    "### Landing the Data into Storage (Hadoop) and Querying\n",
    "\n",
    "Our next step wil be landing the assessments dataframe into storage (Hadoop) to make it querieable in an efficient way. We will use a Parquet format for that. We will also create a Spark TempTable (View) for analysis:\n",
    "```python\n",
    "assess_df.write.parquet(\"/tmp/assess_df\")\n",
    "assess_df.registerTempTable('assessments_new')\n",
    "```\n",
    "\n",
    "Now we can start using SparkSQL to query the data and answer some questions. First, we will examine the data and see what our main challenges are.\n",
    "As a start, let's look at the dataframe as a whole, and then look into the \"certification\" and \"max_attemps\" fields:\n",
    "```sql\n",
    "spark.sql(\"select * from assessments_new limit 10\").show()\n",
    "spark.sql(\"SELECT certification, count(*) from assessments_new GROUP BY certification\").show()\n",
    "spark.sql(\"SELECT max_attempts, count(*) from assessments_new GROUP BY max_attempts\").show()\n",
    "```\n",
    "\n",
    "##### We can already notice (at least) 3 issues with the dataset:\n",
    "* The main challenge will be the nested arrays in the \"sequences\" field. This is due to the multiple questions in each sequence.\n",
    "* There are null values in several of our fields\n",
    "* Some of the exam_name data is aligned to the right and some is aligned to the left\n",
    "\n",
    "In order to really prepare the dataset for anlysis, we would want to unwrap the nested arrays while choosing only specific fields we are interested in from them. We will not do this right now for this project.\n",
    "\n",
    "##### Let's begin looking into some questions with SQL and store some of them into Hadoop:\n",
    "\n",
    "1. How many assessments are in the dataset?\n",
    "```sql\n",
    "spark.sql(\"select COUNT(*) FROM assessments_new\").show()\n",
    "```\n",
    "\n",
    "\n",
    "2. How many people took Learning Git?\n",
    "```sql\n",
    "spark.sql(\"select COUNT(*) FROM assessments_new WHERE exam_name LIKE '%Learning%Git%'\").show()\n",
    "```\n",
    "\n",
    "\n",
    "3. What is the least common course taken? And the most common? (We will store this view)\n",
    "```sql\n",
    "spark.sql(\"SELECT exam_name, COUNT(*) as exam_count FROM assessments_new GROUP BY exam_name ORDER BY exam_count DESC\").show(3000)\n",
    "course_count = spark.sql(\"SELECT exam_name, COUNT(*) as exam_count FROM assessments_new GROUP BY exam_name ORDER BY exam_count DESC\")\n",
    "course_count.write.parquet(\"/tmp/course_count\")\n",
    "``` \n",
    "\n",
    "\n",
    "4. How many certifications were successful? How many weren't?\n",
    "```sql\n",
    "spark.sql(\"SELECT certification, count(*) from assessments_new GROUP BY certification\").show()\n",
    "```\n",
    "\n",
    "\n",
    "5. What was the number instances for each number of maximum attempts?\n",
    "```sql\n",
    "spark.sql(\"SELECT max_attempts, count(*) from assessments_new GROUP BY max_attempts\").show()\n",
    "```\n",
    "\n",
    "\n",
    "6. What is the earliest date and latest date for the assessments in our dataset? (We will store this view)\n",
    "```sql\n",
    "spark.sql(\"SELECT DATE(MAX(started_at)), DATE(MIN(started_at)) from assessments_new\").show()\n",
    "assess_dates = spark.sql(\"SELECT DATE(MAX(started_at)), DATE(MIN(started_at)) from assessments_new\")\n",
    "assess_dates.write.parquet(\"/tmp/assess_dates\")\n",
    "``` \n",
    "\n",
    "We are done with our analysis and data wrangling and can exit Spark:\n",
    "```python\n",
    "exit()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0349106",
   "metadata": {},
   "source": [
    "### Final Steps\n",
    "\n",
    "The last phase will be looking into the results we have saves into Hadoop:\n",
    "```bash\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/course_count\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/assess_dates\n",
    "```\n",
    "\n",
    "Now the data is ready for data scientists, who work for our customers, to run queries on the data, and examine some of the questions we have already looked into.\n",
    "To wrap things up, we will take our Docker-Compose down:\n",
    "```bash\n",
    "docker-compose down\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m78"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
